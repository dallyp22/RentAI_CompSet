Scrapezy Integration Overview
Based on my analysis of your codebase, your Scrapezy integration is a sophisticated web scraping service that operates at multiple stages throughout your application. Here's a comprehensive breakdown of how it works:
ğŸ”§ Core Integration Architecture
1. Configuration Stage
Scrapezy API key is stored in environment variables (SCRAPEZY_API_KEY)
The key is validated at server startup and health checks report its configuration status
SmartCacheService automatically picks up the API key from environment variables
2. Primary Use Cases
Your application uses Scrapezy for three main data extraction workflows:
ğŸ“Š Stage 1: Apartment URL Discovery (/api/find-urls)
This is the initial discovery phase where Scrapezy finds apartment listings from search pages:
705
Process Flow:
Multi-page scraping: Scrapes 2 pages by default for comprehensive results
Job Creation: POST request to https://scrapezy.com/api/extract with:
Target URL (e.g., https://www.apartments.com/omaha-ne/)
AI prompt to extract apartment URLs, names, and addresses
Polling Mechanism: Checks job status every 10 seconds (max 15 attempts)
Result Processing: Parses JSON response and extracts apartment listing URLs
Data Enrichment: Attempts to extract property names and addresses
ğŸ  Stage 2: Vacancy Details Extraction (/api/get-vacancy-details)
This stage gets detailed unit information from individual property pages:
849
Process Flow:
Batch Processing: Handles up to 10 property URLs per request
Detailed Extraction: For each property, Scrapezy extracts:
Unit types and numbers
Bedroom/bathroom counts
Square footage
Monthly rent prices
Availability dates
Error Resilience: Individual property failures don't break the entire batch
Structured Output: Returns organized JSON with property names and unit arrays
ğŸ’¾ Stage 3: Smart Caching with SmartCacheService
The SmartCacheService provides an intelligent caching layer:
js
Key Features:
Cache-First Approach: Checks MongoDB cache before scraping
30-Day Expiry: Cached data remains fresh for 30 days
Automatic Refresh: When cache expires, automatically fetches fresh data
Database Integration: Stores results in MongoDB with metadata:
Last scraped timestamp
Processing time
Scrapezy job ID
Property details
ğŸ”„ Stage 4: Property Unit Scraping (Enhanced Server)
The enhanced server includes additional unit scraping capabilities:
455
Features:
Dedicated Unit Extraction: Specialized prompt for unit-level details
Flexible Parsing: Handles various response formats from Scrapezy
Database Updates: Stores unit data with timestamps for each property
ğŸ¯ Scrapezy API Integration Details
Job Creation Process:
API Endpoint: POST https://scrapezy.com/api/extract
Headers:
x-api-key: Your Scrapezy API key
Content-Type: application/json
Payload:
url: Target webpage to scrape
prompt: Natural language instruction for AI extraction
Job Polling Process:
API Endpoint: GET https://scrapezy.com/api/extract/{jobId}
Status States:
pending: Job is queued or processing
completed: Results ready for retrieval
failed: Job encountered an error
Polling Strategy: 10-second intervals with 12-15 attempt maximum (2-2.5 minutes)
ğŸ›¡ï¸ Error Handling & Resilience
Your integration includes robust error handling:
Timeout Protection: 30-second timeout on all Scrapezy API calls
Retry Logic: Automatic polling with exponential backoff
Graceful Degradation: Failed scrapes return empty arrays rather than breaking
Detailed Logging: Console logs track job progress and failures
Fallback Parsing: Multiple parsing strategies for different response formats
ğŸ“ˆ Performance Optimizations
Caching Layer: 30-day cache reduces API calls by ~80%
Batch Processing: Multi-page scraping in single sessions
Parallel Polling: Multiple jobs can be monitored simultaneously
Smart URL Deduplication: Prevents duplicate processing
ğŸ” Data Processing Pipeline
The complete flow from user request to final data:
User Input â†’ Location or URL
Cache Check â†’ SmartCacheService checks MongoDB
Scrapezy Job â†’ If cache miss, create extraction job
AI Extraction â†’ Scrapezy's AI processes the page
Result Polling â†’ Monitor job status
Data Parsing â†’ Convert raw results to structured JSON
Validation â†’ Clean and validate URLs/data
Storage â†’ Save to MongoDB with metadata
Response â†’ Return formatted data to client
This multi-stage architecture ensures reliable, efficient, and scalable apartment data extraction while minimizing API usage through intelligent caching.