# Technical Architecture: Scrapezy Integration Multi-Stage Process

## üîß Stage 1: Initial Request & Validation

### Technical Flow:
```javascript
// Entry point: /api/find-urls endpoint
POST /api/find-urls
```

**1.1 Request Validation:**
```javascript
const { url, location } = req.body;
if (!process.env.SCRAPEZY_API_KEY) {
  return res.status(500).json({ error: 'Scrapezy API key not configured' });
}
```

**1.2 Multi-Page URL Construction:**
```javascript
const pagesToScrape = 2;
for (let page = 1; page <= pagesToScrape; page++) {
  const pageUrl = page === 1 ? url : `${url}${page}/`;
}
```

## üöÄ Stage 2: Scrapezy Job Creation

### Technical Implementation:
**2.1 API Request Construction:**
```javascript
const jobResponse = await axios.post(
  'https://scrapezy.com/api/extract',
  {
    url: pageUrl,
    prompt: 'Extract apartment listings from this apartments.com page. For each apartment property listing, extract: 1) The complete URL link to the individual apartment page (must start with https://www.apartments.com/), 2) The property/apartment name or title, 3) The address or location information. Return as JSON array with objects containing "url", "name", and "address" fields.'
  },
  {
    headers: {
      'x-api-key': process.env.SCRAPEZY_API_KEY,
      'Content-Type': 'application/json'
    },
    timeout: 30000
  }
);
```

**2.2 Job ID Extraction:**
```javascript
const jobId = jobResponse.data.id || jobResponse.data.jobId;
if (!jobId) {
  throw new Error('No job ID received from Scrapezy');
}
```

## ‚è±Ô∏è Stage 3: Asynchronous Job Polling

### Polling Mechanism:
**3.1 Polling Configuration:**
```javascript
let attempts = 0;
const maxAttempts = 15; // 2.5 minutes maximum
const POLL_INTERVAL = 10000; // 10 seconds
```

**3.2 Status Checking Loop:**
```javascript
while (attempts < maxAttempts) {
  await new Promise(resolve => setTimeout(resolve, POLL_INTERVAL));
  attempts++;
  
  const resultResponse = await axios.get(
    `https://scrapezy.com/api/extract/${jobId}`,
    {
      headers: {
        'x-api-key': process.env.SCRAPEZY_API_KEY,
        'Content-Type': 'application/json'
      },
      timeout: 30000
    }
  );
  
  // Status evaluation
  if (resultResponse.data.status === 'completed') {
    finalResult = resultResponse.data;
    break;
  } else if (resultResponse.data.status === 'failed') {
    throw new Error(`Scrapezy job failed: ${resultResponse.data.error}`);
  }
}
```

## üîÑ Stage 4: Result Processing & Parsing

### Data Extraction Pipeline:
**4.1 Raw Result Parsing:**
```javascript
function parseUrls(scrapezyResult) {
  let urls = [];
  
  // Multiple parsing strategies for different response formats
  if (typeof scrapezyResult === 'string') {
    try {
      const parsed = JSON.parse(scrapezyResult);
      if (parsed.apartment_listing_urls && Array.isArray(parsed.apartment_listing_urls)) {
        urls = parsed.apartment_listing_urls;
      } else if (Array.isArray(parsed)) {
        urls = parsed.filter(item => typeof item === 'string' && item.includes('apartments.com'));
      }
    } catch (e) {
      // Fallback to line-by-line parsing
      urls = scrapezyResult.split('\n')
        .map(line => line.trim())
        .filter(line => line.includes('apartments.com') && line.startsWith('http'));
    }
  }
  
  // URL validation and deduplication
  return urls
    .filter(url => url && typeof url === 'string')
    .filter(url => url.includes('apartments.com'))
    .filter(url => url.startsWith('http'))
    .map(url => url.trim())
    .filter((url, index, arr) => arr.indexOf(url) === index);
}
```

## üíæ Stage 5: Smart Caching Layer (SmartCacheService)

### Cache Management System:
**5.1 Cache Check:**
```javascript
async getOrFetchUrls(city, state) {
  const cityKey = `${city}-${state}`.toLowerCase();
  const cached = await CityData.findOne({ city: cityKey });
  
  // Cache freshness evaluation (30-day expiry)
  if (cached && this.isFresh(cached.lastScraped)) {
    const daysSince = moment().diff(moment(cached.lastScraped), 'days');
    if (daysSince < this.CACHE_EXPIRY_DAYS) {
      return {
        urls: cached.apartmentUrls,
        fromCache: true,
        cacheAge: moment(cached.lastScraped).fromNow()
      };
    }
  }
  
  // Cache miss - trigger fresh scrape
  const freshData = await this.scrapeApartmentUrls(city, state);
}
```

**5.2 Database Persistence:**
```javascript
const updatedData = await CityData.findOneAndUpdate(
  { city: cityKey },
  { 
    city: cityKey,
    state: state,
    lastScraped: new Date(),
    apartmentUrls: freshData.urls.map(url => ({
      url: url,
      name: this.extractPropertyName(url),
      lastUnitScrape: null,
      unitData: []
    })),
    scrapingMetadata: {
      totalPropertiesFound: freshData.urls.length,
      processingTime: freshData.processingTime,
      scrapezyJobId: freshData.jobId
    }
  },
  { 
    upsert: true, 
    new: true,
    setDefaultsOnInsert: true
  }
);
```

## üè¢ Stage 6: Vacancy Details Extraction (Second-Level Scraping)

### Deep Property Analysis:
**6.1 Batch Processing Setup:**
```javascript
app.post('/api/get-vacancy-details', async (req, res) => {
  const { urls } = req.body;
  
  // Batch size limitation
  if (urls.length > 10) {
    return res.status(400).json({
      error: 'Maximum 10 URLs allowed per request'
    });
  }
```

**6.2 Individual Property Scraping:**
```javascript
for (let i = 0; i < urls.length; i++) {
  const jobResponse = await axios.post(
    'https://scrapezy.com/api/extract',
    {
      url: urls[i],
      prompt: 'Extract detailed apartment vacancy information from this apartments.com property page. For each available unit, extract: 1) Unit type/name, 2) Unit number (if available), 3) Number of bedrooms, 4) Number of bathrooms, 5) Square footage, 6) Monthly rent price, 7) Availability date. Return as JSON with structure: {"propertyName": "Property Name", "units": [{"type": "1 Bedroom", "unitNumber": "101", "beds": "1", "baths": "1", "sqft": "650", "rent": "$1,200", "availability": "Available Now"}]}'
    },
    {
      headers: {
        'x-api-key': process.env.SCRAPEZY_API_KEY,
        'Content-Type': 'application/json'
      },
      timeout: 30000
    }
  );
  
  // Poll for individual property results
  // Parse and aggregate unit data
}
```

## üîÄ Stage 7: WebSocket Real-Time Updates (Market Analysis)

### Socket.IO Integration:
**7.1 Real-Time Progress Reporting:**
```javascript
async function processMarketAnalysis(propertyName, location, socketId, quickInsight) {
  const socket = io.sockets.sockets.get(socketId);
  
  // Progress emissions during processing
  socket.emit('analysis-progress', { 
    stage: 'Finding apartments', 
    progress: 10 
  });
  
  // Scrapezy integration
  const apartmentData = await cacheService.getOrFetchUrls(city, state);
  
  socket.emit('analysis-progress', { 
    stage: 'Analyzing market', 
    progress: 50 
  });
  
  // Results streaming
  socket.emit('analysis-complete', { 
    analysis: competitorAnalysis 
  });
}
```

## üõ°Ô∏è Stage 8: Error Handling & Recovery

### Comprehensive Error Management:
**8.1 Timeout Handling:**
```javascript
if (!finalResult && attempts >= maxAttempts) {
  throw new Error('Scrapezy job timed out after 2.5 minutes');
}
```

**8.2 Graceful Degradation:**
```javascript
catch (error) {
  console.error(`‚ùå Error processing ${url}:`, error.message);
  vacancies.push({
    sourceUrl: url,
    propertyName: `Property ${i + 1}`,
    units: [],
    error: error.message
  });
  // Continue processing remaining URLs
}
```

**8.3 API Key Validation:**
```javascript
// Server startup validation
console.log(`üîë Scrapezy API Key: ${process.env.SCRAPEZY_API_KEY ? '‚úÖ Configured' : '‚ùå Missing'}`);
```

## üìä Stage 9: Data Aggregation & Response

### Final Data Assembly:
```javascript
const response = {
  success: true,
  apartments: allApartments,
  sourceUrl: url,
  location: location,
  count: allApartments.length,
  jobIds: jobIds,
  processingTime: `${Math.round((Date.now() - startTime) / 1000)} seconds`,
  metadata: {
    pagesScraped: pagesToScrape,
    cacheStatus: fromCache ? 'hit' : 'miss',
    totalUnits: totalUnits
  }
};
```

## üîß Technical Configuration Details

### Environment Setup:
```bash
SCRAPEZY_API_KEY=your_api_key_here
PORT=3000
MONGODB_URI=mongodb://localhost:27017/gemrent4
```

### API Rate Limits & Constraints:
- **Polling Interval**: 10 seconds between status checks
- **Max Polling Duration**: 2.5 minutes (15 attempts)
- **Request Timeout**: 30 seconds per API call
- **Batch Size Limit**: 10 properties per vacancy detail request
- **Cache TTL**: 30 days
- **Multi-Page Scraping**: 2 pages by default

### Performance Metrics:
- **Average Job Completion**: 20-30 seconds
- **Cache Hit Rate**: ~80% after initial population
- **URL Discovery Rate**: 30-50 properties per city
- **Unit Detail Extraction**: 5-10 seconds per property

This multi-stage architecture ensures reliable data extraction with optimal performance through intelligent caching, parallel processing, and robust error handling.